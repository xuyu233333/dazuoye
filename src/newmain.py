import os
import sys
import json
import time
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Any, Optional
from tqdm import tqdm  # è¿›åº¦æ¡åº“
import argparse

# å¯¼å…¥è‡ªå®šä¹‰åˆ†æå™¨
from analyzer.ast_parser import ASTParser
from analyzer.metrics_calculator import MetricsCalculator
from analyzer.style_checker import StyleChecker
from analyzer.visualizer import Visualizer
from analyzer.complexity_analyzer import AdvancedComplexityAnalyzer
from analyzer.dependency_analyzer import DependencyAnalyzer
from analyzer.security_analyzer import SecurityAnalyzer
from analyzer.test_coverage_analyzer import TestCoverageAnalyzer

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class Config:
    """é…ç½®æ–‡ä»¶ç±»"""
    
    DEFAULT_CONFIG = {
        'analysis': {
            'max_files': 200,
            'skip_tests': True,
            'skip_large_files': True,
            'max_file_size_mb': 1,
            'exclude_patterns': ['__pycache__', '.git', '.svn', '.hg'],
            'include_extensions': ['.py', '.pyx']
        },
        'metrics': {
            'enable_complexity': True,
            'enable_security': True,
            'enable_dependency': True,
            'enable_coverage': False,
            'min_lines_for_analysis': 5
        },
        'output': {
            'reports_dir': 'reports',
            'save_raw_data': True,
            'generate_html': True,
            'generate_pdf': False,
            'verbose': False
        }
    }
    
    @classmethod
    def load(cls, config_path: Optional[str] = None):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config = cls.DEFAULT_CONFIG.copy()
        
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    # æ·±åº¦åˆå¹¶é…ç½®
                    cls._deep_update(config, user_config)
                    print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            except Exception as e:
                print(f"âš ï¸  é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        return config
    
    @staticmethod
    def _deep_update(original: Dict, update: Dict):
        """æ·±åº¦æ›´æ–°å­—å…¸"""
        for key, value in update.items():
            if key in original and isinstance(original[key], dict) and isinstance(value, dict):
                Config._deep_update(original[key], value)
            else:
                original[key] = value

class FileCollector:
    """æ–‡ä»¶æ”¶é›†å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.analysis_config = config['analysis']
        self.metrics_config = config['metrics']
    
    def collect_files(self, base_path: Path) -> List[Path]:
        """æ”¶é›†è¦åˆ†æçš„æ–‡ä»¶"""
        all_files = []
        include_exts = self.analysis_config['include_extensions']
        exclude_patterns = self.analysis_config['exclude_patterns']
        
        print(f"ğŸ“ æ‰«æç›®å½•: {base_path}")
        
        for ext in include_exts:
            for file_path in base_path.rglob(f"*{ext}"):
                # æ£€æŸ¥æ’é™¤æ¨¡å¼
                if any(pattern in str(file_path) for pattern in exclude_patterns):
                    continue
                
                # æ£€æŸ¥æ˜¯å¦è·³è¿‡æµ‹è¯•æ–‡ä»¶
                if self.analysis_config['skip_tests'] and self._is_test_file(file_path):
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                if self.analysis_config['skip_large_files']:
                    try:
                        file_size_mb = file_path.stat().st_size / (1024 * 1024)
                        if file_size_mb > self.analysis_config['max_file_size_mb']:
                            continue
                    except:
                        pass
                
                all_files.append(file_path)
        
        # é™åˆ¶æ–‡ä»¶æ•°é‡
        max_files = self.analysis_config['max_files']
        if max_files and len(all_files) > max_files:
            print(f"ğŸ“Š æ–‡ä»¶æ•°é‡é™åˆ¶: {max_files} (å…±æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶)")
            all_files = all_files[:max_files]
        
        print(f"âœ… æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶ç”¨äºåˆ†æ")
        return all_files
    
    def _is_test_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæµ‹è¯•æ–‡ä»¶"""
        path_str = str(file_path).lower()
        name = file_path.name.lower()
        
        # æ£€æŸ¥æ–‡ä»¶åæ¨¡å¼
        test_patterns = [
            'test_', '_test.', 'test.', '_test_',
            'tests/', 'test/', '__test__'
        ]
        
        return any(pattern in path_str for pattern in test_patterns)

class PandasCodeAnalyzer:
    def __init__(self, config: Dict):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config = config
        self.output_config = config['output']
        self.metrics_config = config['metrics']
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.reports_dir = Path(self.output_config['reports_dir'])
        self.reports_dir.mkdir(exist_ok=True, parents=True)
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.ast_parser = ASTParser()
        self.metrics_calc = MetricsCalculator()
        self.style_checker = StyleChecker()
        self.visualizer = Visualizer()
        
        # é«˜çº§åˆ†æå™¨
        self.complexity_analyzer = AdvancedComplexityAnalyzer()
        self.dependency_analyzer = DependencyAnalyzer()
        self.security_analyzer = SecurityAnalyzer()
        self.test_analyzer = TestCoverageAnalyzer()
        
        # ç»“æœå­˜å‚¨
        self.results_df = pd.DataFrame()
        self.complexity_results = []
        self.security_results = []
        self.dependency_results = []
        self.test_results = []
        
        print("ğŸ”§ åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_directory(self, source_path: Path) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªç›®å½•"""
        print(f"ğŸš€ å¼€å§‹åˆ†æ: {source_path}")
        start_time = time.time()
        
        # æ”¶é›†æ–‡ä»¶
        collector = FileCollector(self.config)
        files_to_analyze = collector.collect_files(source_path)
        
        if not files_to_analyze:
            print("âŒ æœªæ‰¾åˆ°å¯åˆ†æçš„æ–‡ä»¶")
            return {}
        
        # åˆ†ææ–‡ä»¶
        analysis_results = self._analyze_files(files_to_analyze)
        
        # æ‰§è¡Œé«˜çº§åˆ†æ
        self._perform_advanced_analysis(analysis_results, files_to_analyze)
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_comprehensive_report(analysis_results)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = time.time() - start_time
        stats = self._calculate_statistics(analysis_results, elapsed_time)
        
        return stats
    
    def _analyze_files(self, files: List[Path]) -> List[Dict]:
        """åˆ†ææ–‡ä»¶åˆ—è¡¨"""
        results = []
        errors = []
        
        print("ğŸ“Š å¼€å§‹æ–‡ä»¶åˆ†æ...")
        
        for file_path in tqdm(files, desc="åˆ†æè¿›åº¦", unit="æ–‡ä»¶"):
            try:
                result = self._analyze_single_file(file_path)
                if result:  # åªæ·»åŠ æœ‰æ•ˆç»“æœ
                    results.append(result)
            except Exception as e:
                error_info = {
                    'file': str(file_path),
                    'error': str(e)
                }
                errors.append(error_info)
                
                if self.output_config['verbose']:
                    print(f"âŒ åˆ†æå¤±è´¥: {file_path} - {e}")
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        if errors:
            self._save_errors(errors)
        
        print(f"âœ… æˆåŠŸåˆ†æ {len(results)}/{len(files)} ä¸ªæ–‡ä»¶")
        return results
    
    def _analyze_single_file(self, file_path: Path) -> Optional[Dict]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                code = f.read()
            
            # è·³è¿‡è¿‡å°çš„æ–‡ä»¶
            if len(code.strip().split('\n')) < self.metrics_config['min_lines_for_analysis']:
                return None
            
            # è§£æAST
            ast_tree = self.ast_parser.parse_code(code)
            
            # åŸºç¡€æŒ‡æ ‡
            metrics = self.metrics_calc.calculate_file_metrics(code, ast_tree)
            
            # æ–‡ä»¶ä¿¡æ¯
            metrics['file_path'] = str(file_path)
            metrics['file_name'] = file_path.name
            metrics['file_size_kb'] = len(code) / 1024
            metrics['lines_of_code'] = len(code.split('\n'))
            
            # ä»£ç è§„èŒƒæ£€æŸ¥
            try:
                style_issues = self.style_checker.check_file(str(file_path))
                metrics['style_issues'] = len(style_issues)
                if style_issues:
                    metrics['style_issue_details'] = style_issues[:10]  # åªä¿ç•™å‰10ä¸ª
            except:
                metrics['style_issues'] = 0
            
            # å‡½æ•°ä¿¡æ¯
            functions = self.ast_parser.extract_functions(ast_tree)
            metrics['function_count'] = len(functions)
            if functions:
                metrics['avg_function_lines'] = sum(f['lines'] for f in functions) / len(functions)
                metrics['max_function_lines'] = max(f['lines'] for f in functions)
                metrics['avg_function_args'] = sum(f['arg_count'] for f in functions) / len(functions)
                metrics['functions_with_docstring'] = sum(1 for f in functions if f['has_docstring'])
            
            # å¯¼å…¥ä¿¡æ¯
            imports = self.ast_parser.extract_imports(ast_tree)
            metrics['import_count'] = len(imports)
            
            # é«˜çº§å¤æ‚åº¦åˆ†æ
            if self.metrics_config['enable_complexity']:
                try:
                    complexity = self.complexity_analyzer.analyze_complexity(code, ast_tree)
                    metrics.update(complexity)
                except Exception as e:
                    if self.output_config['verbose']:
                        print(f"å¤æ‚åº¦åˆ†æå¤±è´¥ {file_path}: {e}")
            
            # å®‰å…¨åˆ†æ
            if self.metrics_config['enable_security']:
                try:
                    security = self.security_analyzer.analyze_file_security(str(file_path), code)
                    metrics.update({
                        'security_score': security['security_score'],
                        'security_issues': security['total_issues'],
                        'high_risk_issues': security['high_risk_issues']
                    })
                except Exception as e:
                    if self.output_config['verbose']:
                        print(f"å®‰å…¨åˆ†æå¤±è´¥ {file_path}: {e}")
            
            return metrics
            
        except Exception as e:
            if self.output_config['verbose']:
                print(f"åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    def _perform_advanced_analysis(self, analysis_results: List[Dict], files: List[Path]):
        """æ‰§è¡Œé«˜çº§åˆ†æ"""
        print("ğŸ” æ‰§è¡Œé«˜çº§åˆ†æ...")
        
        # ä¾èµ–åˆ†æ
        if self.metrics_config['enable_dependency']:
            print("  ğŸ“¦ åˆ†æä¾èµ–å…³ç³»...")
            self.dependency_results = []
            for result in analysis_results:
                try:
                    with open(result['file_path'], 'r', encoding='utf-8') as f:
                        code = f.read()
                    deps = self.dependency_analyzer.analyze_file(result['file_path'], code)
                    self.dependency_results.append(deps)
                except Exception as e:
                    if self.output_config['verbose']:
                        print(f"ä¾èµ–åˆ†æå¤±è´¥ {result.get('file_path', 'unknown')}: {e}")
                    continue
            
            if self.dependency_results:
                try:
                    # ä¿®å¤ï¼šä¸ä¾èµ–å¯è§†åŒ–æ–¹æ³•ï¼Œåªç”ŸæˆæŠ¥å‘Š
                    dep_df = pd.DataFrame(self.dependency_results)
                    dep_output_path = self.reports_dir / 'dependency_analysis.csv'
                    dep_df.to_csv(dep_output_path, index=False)
                    print(f"  âœ… ä¾èµ–åˆ†æç»“æœä¿å­˜è‡³: {dep_output_path}")
                    
                    # å°è¯•æ„å»ºä¾èµ–å›¾ä½†ä¸å¼ºåˆ¶å¯è§†åŒ–
                    try:
                        # æ„å»ºä¾èµ–å›¾
                        import networkx as nx
                        from collections import defaultdict
                        
                        G = nx.DiGraph()
                        for result in self.dependency_results:
                            file_name = Path(result['file']).stem
                            G.add_node(file_name, **result)
                            
                            for imp in result.get('import_details', []):
                                if imp['type'] == 'from_import' and imp['module']:
                                    source_module = Path(imp['module']).stem
                                    if source_module in G.nodes:
                                        G.add_edge(file_name, source_module)
                        
                        # ä¿å­˜ä¾èµ–å›¾æ•°æ®
                        graph_data_path = self.reports_dir / 'dependency_graph.gpickle'
                        nx.write_gpickle(G, graph_data_path)
                        print(f"  âœ… ä¾èµ–å›¾æ•°æ®ä¿å­˜è‡³: {graph_data_path}")
                        
                        # å°è¯•ç”Ÿæˆç®€åŒ–å¯è§†åŒ–ï¼ˆä¸å¼ºåˆ¶ä¾èµ–matplotlibï¼‰
                        self._generate_simple_dependency_report(G)
                        
                    except Exception as graph_error:
                        print(f"  âš ï¸  ä¾èµ–å›¾ç”Ÿæˆå¤±è´¥: {graph_error}")
                        
                except Exception as e:
                    print(f"  âŒ ä¾èµ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        
        # æµ‹è¯•è¦†ç›–ç‡åˆ†æ
        if self.metrics_config['enable_coverage']:
            print("  âœ… åˆ†ææµ‹è¯•è¦†ç›–ç‡...")
            try:
                test_files, non_test_files = self.test_analyzer.identify_test_files(files)
                test_stats = self.test_analyzer.analyze_test_structure(test_files)
                
                # è¿è¡Œè¦†ç›–ç‡å·¥å…·
                try:
                    coverage_data = self.test_analyzer.run_coverage_tool()
                    test_df = self.test_analyzer.generate_test_report(test_stats, coverage_data)
                    self.test_results = test_stats
                except Exception as cov_error:
                    print(f"  âš ï¸  è¦†ç›–ç‡å·¥å…·è¿è¡Œå¤±è´¥: {cov_error}")
                    test_df = self.test_analyzer.generate_test_report(test_stats)
                    self.test_results = test_stats
            except Exception as test_error:
                print(f"  âŒ æµ‹è¯•åˆ†æå¤±è´¥: {test_error}")
    
    def _generate_simple_dependency_report(self, graph):
        """ç”Ÿæˆç®€åŒ–çš„ä¾èµ–æŠ¥å‘Šï¼ˆä¸ä¾èµ–matplotlibï¼‰"""
        try:
            import networkx as nx
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            metrics = {
                'total_nodes': len(graph.nodes()),
                'total_edges': len(graph.edges()),
                'density': nx.density(graph) if len(graph.nodes()) > 1 else 0,
            }
            
            # è®¡ç®—å…¥åº¦å’Œå‡ºåº¦
            in_degrees = dict(graph.in_degree())
            out_degrees = dict(graph.out_degree())
            
            if in_degrees:
                metrics['avg_in_degree'] = sum(in_degrees.values()) / len(in_degrees)
                metrics['max_in_degree'] = max(in_degrees.values())
                # æœ€é«˜å…¥åº¦çš„æ¨¡å—
                top_in_degree = sorted(in_degrees.items(), key=lambda x: x[1], reverse=True)[:5]
                metrics['top_in_dependent_modules'] = top_in_degree
            
            if out_degrees:
                metrics['avg_out_degree'] = sum(out_degrees.values()) / len(out_degrees)
                metrics['max_out_degree'] = max(out_degrees.values())
                # æœ€é«˜å‡ºåº¦çš„æ¨¡å—
                top_out_degree = sorted(out_degrees.items(), key=lambda x: x[1], reverse=True)[:5]
                metrics['top_dependent_modules'] = top_out_degree
            
            # æ£€æµ‹å¾ªç¯ä¾èµ–
            try:
                cycles = list(nx.simple_cycles(graph))
                metrics['circular_dependencies'] = len(cycles)
                if cycles:
                    metrics['cycle_details'] = [', '.join(cycle) for cycle in cycles[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ª
            except:
                metrics['circular_dependencies'] = 0
            
            # ä¿å­˜æŒ‡æ ‡
            metrics_path = self.reports_dir / 'dependency_metrics.json'
            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            
            print(f"  âœ… ä¾èµ–æŒ‡æ ‡ä¿å­˜è‡³: {metrics_path}")
            
        except Exception as e:
            print(f"  âš ï¸  ç”Ÿæˆä¾èµ–æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _generate_comprehensive_report(self, analysis_results: List[Dict]):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("ğŸ“„ ç”ŸæˆæŠ¥å‘Š...")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        if self.output_config['save_raw_data'] and analysis_results:
            self.results_df = pd.DataFrame(analysis_results)
            self.results_df.to_csv(self.reports_dir / 'raw_analysis_data.csv', index=False)
            print(f"  ğŸ“Š åŸå§‹æ•°æ®ä¿å­˜è‡³: {self.reports_dir}/raw_analysis_data.csv")
        else:
            print("  âš ï¸  æ— åˆ†æç»“æœï¼Œè·³è¿‡åŸå§‹æ•°æ®ä¿å­˜")
            return
        
        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        if self.output_config['generate_html'] and not self.results_df.empty:
            try:
                self.visualizer.plot_complexity_distribution(self.results_df)
                self.visualizer.plot_style_issues_by_module(self.results_df)
                self.visualizer.plot_function_metrics(self.results_df)
                self.visualizer.generate_html_report(self.results_df)
                print("  âœ… å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            except Exception as e:
                print(f"  âŒ å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        
        # ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
        self._generate_text_summary(analysis_results)
        
        # ç”Ÿæˆé…ç½®æ–‡ä»¶
        self._generate_config_summary()
    
    def _generate_text_summary(self, analysis_results: List[Dict]):
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
        if not analysis_results:
            return
        
        summary = {
            "analysis_summary": self._calculate_statistics(analysis_results, 0),
            "top_complex_files": self._get_top_complex_files(analysis_results),
            "security_alerts": self._get_security_alerts(analysis_results),
            "recommendations": self._generate_recommendations(analysis_results)
        }
        
        # ä¿å­˜JSONæ ¼å¼
        json_path = self.reports_dir / 'analysis_summary.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜Markdownæ ¼å¼
        md_path = self.reports_dir / 'README.md'
        self._generate_markdown_report(summary, md_path)
    
    def _generate_markdown_report(self, summary: Dict, output_path: Path):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        content = f"""# ä»£ç è´¨é‡åˆ†ææŠ¥å‘Š

## ğŸ“Š åˆ†ææ¦‚è§ˆ
- **åˆ†ææ–‡ä»¶æ•°**: {summary['analysis_summary']['total_files']}
- **å¹³å‡å¤æ‚åº¦**: {summary['analysis_summary']['avg_complexity']:.2f}
- **å¹³å‡ç»´æŠ¤æŒ‡æ•°**: {summary['analysis_summary']['avg_maintainability']:.2f}
- **å®‰å…¨åˆ†æ•°**: {summary['analysis_summary']['avg_security_score']:.2f}

## âš ï¸ å¤æ‚åº¦æœ€é«˜çš„æ–‡ä»¶
"""
        
        for i, file_info in enumerate(summary['top_complex_files'][:5], 1):
            content += f"{i}. **{file_info['file_name']}** - å¤æ‚åº¦: {file_info['cyclomatic_complexity']:.2f}\n"
        
        content += "\n## ğŸ”’ å®‰å…¨è­¦æŠ¥\n"
        if summary['security_alerts']:
            for alert in summary['security_alerts'][:3]:
                content += f"- **{alert['file_name']}**: {alert['high_risk_issues']} ä¸ªé«˜é£é™©é—®é¢˜\n"
        else:
            content += "âœ… æœªå‘ç°é«˜é£é™©å®‰å…¨é—®é¢˜\n"
        
        content += "\n## ğŸ’¡ æ”¹è¿›å»ºè®®\n"
        for rec in summary['recommendations']:
            content += f"- {rec}\n"
        
        content += f"""
## ğŸ“ æ–‡ä»¶è¯¦æƒ…
å®Œæ•´åˆ†æç»“æœè§: `{self.reports_dir}/raw_analysis_data.csv`

## ğŸ“ˆ å¯è§†åŒ–æŠ¥å‘Š
HTMLæŠ¥å‘Š: `{self.reports_dir}/analysis_report.html`
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def _generate_config_summary(self):
        """ç”Ÿæˆé…ç½®æ–‡ä»¶æ‘˜è¦"""
        config_summary = {
            'analysis_config': self.config['analysis'],
            'metrics_config': self.config['metrics'],
            'output_config': self.config['output'],
            'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        config_path = self.reports_dir / 'config_summary.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_summary, f, indent=2)
    
    def _save_errors(self, errors: List[Dict]):
        """ä¿å­˜é”™è¯¯ä¿¡æ¯"""
        if errors:
            errors_path = self.reports_dir / 'analysis_errors.json'
            with open(errors_path, 'w', encoding='utf-8') as f:
                json.dump(errors, f, indent=2)
            print(f"âš ï¸  åˆ†æé”™è¯¯ä¿å­˜è‡³: {errors_path}")
    
    def _calculate_statistics(self, results: List[Dict], elapsed_time: float) -> Dict:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not results:
            return {}
        
        df = pd.DataFrame(results)
        
        stats = {
            'total_files': len(results),
            'total_lines_of_code': int(df['lines_of_code'].sum()) if 'lines_of_code' in df.columns else 0,
            'analysis_time_seconds': round(elapsed_time, 2),
            'files_per_second': round(len(results) / elapsed_time, 2) if elapsed_time > 0 else 0
        }
        
        # å¯é€‰æŒ‡æ ‡
        if 'cyclomatic_complexity' in df.columns:
            stats['avg_complexity'] = float(df['cyclomatic_complexity'].mean())
        
        if 'maintainability_index' in df.columns:
            stats['avg_maintainability'] = float(df['maintainability_index'].mean())
        
        if 'security_score' in df.columns:
            stats['avg_security_score'] = float(df['security_score'].mean())
        
        if 'style_issues' in df.columns:
            stats['total_style_issues'] = int(df['style_issues'].sum())
        
        if 'function_count' in df.columns:
            stats['avg_function_count'] = float(df['function_count'].mean())
        
        return stats
    
    def _get_top_complex_files(self, results: List[Dict]) -> List[Dict]:
        """è·å–å¤æ‚åº¦æœ€é«˜çš„æ–‡ä»¶"""
        if not results:
            return []
        
        df = pd.DataFrame(results)
        if 'cyclomatic_complexity' not in df.columns:
            return []
        
        top_files = df.nlargest(10, 'cyclomatic_complexity')
        return top_files[['file_path', 'file_name', 'cyclomatic_complexity']].to_dict('records')
    
    def _get_security_alerts(self, results: List[Dict]) -> List[Dict]:
        """è·å–å®‰å…¨è­¦æŠ¥"""
        alerts = []
        
        for result in results:
            high_risk = result.get('high_risk_issues', 0)
            if high_risk > 0:
                alerts.append({
                    'file_path': result['file_path'],
                    'file_name': result.get('file_name', 'Unknown'),
                    'high_risk_issues': high_risk,
                    'security_score': result.get('security_score', 100)
                })
        
        return sorted(alerts, key=lambda x: x['high_risk_issues'], reverse=True)
    
    def _generate_recommendations(self, results: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if not results:
            return ["æ— æ³•ç”Ÿæˆå»ºè®®ï¼šæ²¡æœ‰åˆ†æç»“æœ"]
        
        df = pd.DataFrame(results)
        
        # å¤æ‚åº¦å»ºè®®
        if 'cyclomatic_complexity' in df.columns:
            complex_count = len(df[df['cyclomatic_complexity'] > 20])
            if complex_count > 0:
                recommendations.append(f"é‡æ„ {complex_count} ä¸ªé«˜å¤æ‚åº¦å‡½æ•°ï¼ˆå¤æ‚åº¦ > 20ï¼‰")
        
        # å®‰å…¨å»ºè®®
        if 'high_risk_issues' in df.columns:
            high_risk_count = df['high_risk_issues'].sum()
            if high_risk_count > 0:
                recommendations.append(f"ç«‹å³ä¿®å¤ {high_risk_count} ä¸ªé«˜é£é™©å®‰å…¨é—®é¢˜")
        
        # æ–‡æ¡£å»ºè®®
        if 'functions_with_docstring' in df.columns and 'function_count' in df.columns:
            total_functions = df['function_count'].sum()
            documented_functions = df['functions_with_docstring'].sum()
            if total_functions > 0 and documented_functions / total_functions < 0.5:
                recommendations.append("ä¸ºè¶…è¿‡50%çš„å‡½æ•°æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²")
        
        # ä»£ç è§„èŒƒå»ºè®®
        if 'style_issues' in df.columns:
            total_issues = df['style_issues'].sum()
            if total_issues > 100:
                recommendations.append(f"ä¿®å¤ {total_issues} ä¸ªä»£ç è§„èŒƒé—®é¢˜")
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "ä½¿ç”¨ç±»å‹æ³¨è§£æé«˜ä»£ç å¯è¯»æ€§",
            "ä¿æŒå‡½æ•°å•ä¸€èŒè´£åŸåˆ™",
            "æ·»åŠ å•å…ƒæµ‹è¯•è¦†ç›–å…³é”®åŠŸèƒ½",
            "å®šæœŸè¿›è¡Œä»£ç å®¡æŸ¥",
            "ä½¿ç”¨è‡ªåŠ¨åŒ–ä»£ç è´¨é‡æ£€æŸ¥å·¥å…·"
        ])
        
        return recommendations

def find_pandas_source() -> Path:
    """æŸ¥æ‰¾pandasæºä»£ç è·¯å¾„"""
    try:
        # å°è¯•å¯¼å…¥å·²å®‰è£…çš„pandas
        import pandas
        pandas_path = Path(pandas.__file__).parent.parent  # è·å–åŒ…æ ¹ç›®å½•
        print(f"âœ… å‘ç°å·²å®‰è£…çš„pandas: {pandas_path}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Pythonæ–‡ä»¶
        py_files = list(pandas_path.rglob("*.py"))
        if len(py_files) > 10:  # è‡³å°‘è¦æœ‰ä¸€äº›Pythonæ–‡ä»¶
            return pandas_path
        
    except ImportError:
        print("âŒ æœªå®‰è£…pandasï¼Œè¯·å…ˆå®‰è£…: pip install pandas")
    
    # è¿”å›å½“å‰ç›®å½•
    current_dir = Path(".").absolute()
    print(f"âš ï¸  ä½¿ç”¨å½“å‰ç›®å½•: {current_dir}")
    return current_dir

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='é«˜çº§Pythonä»£ç è´¨é‡åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s  # åˆ†æå½“å‰ç›®å½•
  %(prog)s --path /path/to/code --max-files 100
  %(prog)s --config config.json --verbose
        """
    )
    
    parser.add_argument('--path', type=str, help='æºä»£ç è·¯å¾„')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--max-files', type=int, help='æœ€å¤§åˆ†ææ–‡ä»¶æ•°')
    parser.add_argument('--output', type=str, default='reports', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--skip-tests', action='store_true', help='è·³è¿‡æµ‹è¯•æ–‡ä»¶')
    parser.add_argument('--skip-security', action='store_true', help='è·³è¿‡å®‰å…¨åˆ†æ')
    parser.add_argument('--skip-dependency', action='store_true', help='è·³è¿‡ä¾èµ–åˆ†æ')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = Config.load(args.config)
    
    # æ›´æ–°é…ç½®å‚æ•°
    if args.max_files:
        config['analysis']['max_files'] = args.max_files
    
    if args.output:
        config['output']['reports_dir'] = args.output
    
    if args.verbose:
        config['output']['verbose'] = True
    
    if args.skip_tests:
        config['analysis']['skip_tests'] = True
    
    if args.skip_security:
        config['metrics']['enable_security'] = False
    
    if args.skip_dependency:
        config['metrics']['enable_dependency'] = False
    
    # ç¡®å®šåˆ†æè·¯å¾„
    if args.path:
        source_path = Path(args.path)
        if not source_path.exists():
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {source_path}")
            sys.exit(1)
    else:
        source_path = find_pandas_source()
    
    print("=" * 60)
    print("ğŸš€ Pythonä»£ç è´¨é‡åˆ†æå·¥å…·")
    print("=" * 60)
    print(f"ğŸ“ åˆ†æè·¯å¾„: {source_path}")
    print(f"ğŸ“Š æœ€å¤§æ–‡ä»¶æ•°: {config['analysis']['max_files']}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {config['output']['reports_dir']}")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
    analyzer = PandasCodeAnalyzer(config)
    stats = analyzer.analyze_directory(source_path)
    
    # è¾“å‡ºæ‘˜è¦
    if stats:
        print("\n" + "=" * 60)
        print("ğŸ“‹ åˆ†ææ‘˜è¦")
        print("=" * 60)
        print(f"ğŸ“ åˆ†ææ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"ğŸ“ æ€»ä»£ç è¡Œæ•°: {stats['total_lines_of_code']:,}")
        
        if 'avg_complexity' in stats:
            print(f"âš¡ å¹³å‡å¤æ‚åº¦: {stats['avg_complexity']:.2f}")
        
        if 'avg_maintainability' in stats:
            print(f"ğŸ› ï¸  å¹³å‡ç»´æŠ¤æŒ‡æ•°: {stats['avg_maintainability']:.2f}")
        
        if 'avg_security_score' in stats:
            print(f"ğŸ”’ å¹³å‡å®‰å…¨åˆ†æ•°: {stats['avg_security_score']:.2f}")
        
        if 'total_style_issues' in stats:
            print(f"ğŸ“ è§„èŒƒé—®é¢˜æ€»æ•°: {stats['total_style_issues']}")
        
        print(f"â±ï¸  åˆ†æç”¨æ—¶: {stats['analysis_time_seconds']:.2f}ç§’")
        print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {stats['files_per_second']:.2f} æ–‡ä»¶/ç§’")
        print("=" * 60)
        print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆè‡³: {config['output']['reports_dir']}/")
        print("=" * 60)
    else:
        print("âŒ åˆ†æå¤±è´¥ï¼Œæœªç”Ÿæˆç»“æœ")

if __name__ == "__main__":
    main()